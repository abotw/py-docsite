{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed5f6e2",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95301b7",
   "metadata": {},
   "source": [
    "| Home                                           | Desc                                               | Docs                                                   |\n",
    "|------------------------------------------------|----------------------------------------------------|--------------------------------------------------------|\n",
    "| https://www.crummy.com/software/BeautifulSoup/ | Beautiful Soup 可以从 HTML 或 XML 文件中提取数据 | https://www.crummy.com/software/BeautifulSoup/bs4/doc/ |\n",
    "\n",
    "它是写“爬虫”的利器，通常与 requests 或 selenium 配合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9e750",
   "metadata": {},
   "source": [
    "## Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278c6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Example Page\n",
      "Paragraph: This is a sample paragraph.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the HTML file\n",
    "with open(\"example.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "    \n",
    "# type(html_content)\n",
    "# print(html_content)\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# type(soup)\n",
    "\n",
    "# Extract and print the title\n",
    "title = soup.title.text\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "# Extract and print the text inside the <p> tag\n",
    "paragraph = soup.p.text\n",
    "print(f\"Paragraph: {paragraph}\")\n",
    "\n",
    "# Extract and print the text inside each <li> tag in the <ul>\n",
    "list_items = soup.ul.find_all('li')\n",
    "print(\"List Items:\")\n",
    "for item in list_items:\n",
    "    print(item.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de31a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1章已经成功写入 第1章.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.jjwxc.net/onebook.php?novelid=31816&chapterid=2\"\n",
    "\n",
    "html = requests.get(URL)\n",
    "html_content = html.content\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "str_chapter_title = soup.h2.text\n",
    "\n",
    "\n",
    "novel_body = soup.find('div', style='font-size: 16px;line-height: 1.8;padding: 0 19px 25px;font-family: \\'Microsoft YaHei\\', PingFangSC-Regular, HelveticaNeue-Light, \\'Helvetica Neue Light\\', sans-serif !important')\n",
    "\n",
    "novel_content = novelbody.find_all(string=True, recursive=False)\n",
    "mid_novel_content = '\\n'.join(novelbody.find_all(string=True, recursive=False)).replace('\\r\\n', '').replace(' ', '')\n",
    "str_novel_content = re.sub('\\n+', '\\n\\n', mid_novel_content)\n",
    "\n",
    "# print(f'\\u3000\\u3000{str_chapter_title}')\n",
    "# print(str_novel_content)\n",
    "\n",
    "file_path = f\"{str_chapter_title}.txt\"\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(f'\\u3000\\u3000{str_chapter_title}')\n",
    "    file.write(str_novel_content)\n",
    "    \n",
    "print(f'{str_chapter_title}已经成功写入 {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "64062dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1章 has been successfully written to 第1章.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_html_content(url):\n",
    "#     \"\"\"Fetches the HTML content from the given URL.\"\"\"\n",
    "#     response = requests.get(url)\n",
    "#     return response.content\n",
    "\n",
    "# def extract_novel_content(soup):\n",
    "#     \"\"\"Extracts and formats the novel content from the BeautifulSoup object.\"\"\"\n",
    "#     novel_body = soup.find('div', style='font-size: 16px;line-height: 1.8;padding: 0 19px 25px;font-family: \\'Microsoft YaHei\\', PingFangSC-Regular, HelveticaNeue-Light, \\'Helvetica Neue Light\\', sans-serif !important')\n",
    "    \n",
    "#     if novel_body:\n",
    "#         novel_content = '\\n'.join(novel_body.find_all(string=True, recursive=False)).replace('\\r\\n', '').replace(' ', '')\n",
    "#         return re.sub('\\n+', '\\n\\n', novel_content)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# def write_to_file(file_path, chapter_title, novel_content):\n",
    "#     \"\"\"Writes chapter title and novel content to a text file.\"\"\"\n",
    "#     with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#         file.write(f'\\u3000\\u3000{chapter_title}')\n",
    "#         file.write(novel_content)\n",
    "\n",
    "def main():\n",
    "    URL = \"https://www.jjwxc.net/onebook.php?novelid=31816&chapterid=2\"\n",
    "    html_content = get_html_content(URL)\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    chapter_title = soup.h2.text\n",
    "    novel_content = extract_novel_content(soup)\n",
    "\n",
    "    if novel_content:\n",
    "        file_path = f\"{chapter_title}.txt\"\n",
    "        write_to_file(file_path, chapter_title, novel_content)\n",
    "        print(f'{chapter_title} has been successfully written to {file_path}')\n",
    "    else:\n",
    "        print(\"Failed to extract novel content. Check the HTML structure.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "30f13e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=1',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=2',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=3',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=4',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=5',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=6',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=7',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=8',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=9',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=10',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=11',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=12',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=13',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=14',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=15',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=16',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=17',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=18',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=19',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=20',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=21',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=22',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=23',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=24',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=25',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=26',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=27',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=28',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=29',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=30',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=31',\n",
       " 'http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=33']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.jjwxc.net/onebook.php?novelid=31816\"\n",
    "\n",
    "response = requests.get(URL)\n",
    "html_content = response.content\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "target_table = soup.find('table', {'class': 'cytable', 'id': 'oneboolt'})\n",
    "\n",
    "href_list = []\n",
    "\n",
    "target_rows = target_table.find_all('tr', {'itemprop': 'chapter'})\n",
    "\n",
    "for row in target_rows:\n",
    "    a_element = row.find('a', {'itemprop': 'url'})\n",
    "    if a_element:\n",
    "        href_value = a_element.get('href')\n",
    "        href_list.append(href_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec0b5118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Chapter Links:\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=1\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=2\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=3\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=4\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=5\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=6\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=7\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=8\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=9\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=10\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=11\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=12\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=13\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=14\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=15\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=16\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=17\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=18\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=19\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=20\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=21\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=22\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=23\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=24\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=25\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=26\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=27\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=28\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=29\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=30\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=31\n",
      "http://www.jjwxc.net/onebook.php?novelid=31816&chapterid=33\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the target webpage\n",
    "NOVEL_URL = \"https://www.jjwxc.net/onebook.php?novelid=31816\"\n",
    "\n",
    "def get_html_content(url):\n",
    "    \"\"\"\n",
    "    Function to retrieve HTML content from a given URL.\n",
    "    \n",
    "    Args:\n",
    "    - url (str): The URL of the webpage to fetch.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The HTML content of the webpage.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.content\n",
    "\n",
    "def extract_chapter_links(html_content):\n",
    "    \"\"\"\n",
    "    Function to extract chapter links from the HTML content using BeautifulSoup.\n",
    "    \n",
    "    Args:\n",
    "    - html_content (str): The HTML content of the webpage.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of chapter links.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the target table containing chapter information\n",
    "    target_table = soup.find('table', {'class': 'cytable', 'id': 'oneboolt'})\n",
    "    \n",
    "    href_list = []\n",
    "\n",
    "    # Find all rows containing chapter information\n",
    "    target_rows = target_table.find_all('tr', {'itemprop': 'chapter'})\n",
    "\n",
    "    for row in target_rows:\n",
    "        # Find the anchor element within the row\n",
    "        a_element = row.find('a', {'itemprop': 'url'})\n",
    "        if a_element:\n",
    "            # Extract and append the href value to the list\n",
    "            href_value = a_element.get('href')\n",
    "            href_list.append(href_value)\n",
    "    \n",
    "    return href_list\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the script.\n",
    "    \"\"\"\n",
    "    # Retrieve HTML content from the specified URL\n",
    "    html_content = get_html_content(NOVEL_URL)\n",
    "    \n",
    "    # Extract chapter links from the HTML content\n",
    "    chapter_links = extract_chapter_links(html_content)\n",
    "    \n",
    "    # Print the extracted chapter links\n",
    "    print(\"Extracted Chapter Links:\")\n",
    "    for link in chapter_links:\n",
    "        print(link)\n",
    "\n",
    "# Execute the main function if the script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "95eda64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "碧甃沉（完结）\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.jjwxc.net/onebook.php?novelid=31816\"\n",
    "\n",
    "html_content = requests.get(URL).content\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the h1 tag with itemprop=\"name\"\n",
    "h1_tag = soup.find('h1', itemprop='name')\n",
    "\n",
    "# Find the span tag inside the h1 tag with itemprop=\"articleSection\"\n",
    "span_tag = h1_tag.find('span', itemprop='articleSection')\n",
    "\n",
    "# Extract the text from the span tag\n",
    "result = span_tag.text.strip()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def extract_novel_author(self, soup):\n",
    "    try:\n",
    "        # Extract the novel title from the BeautifulSoup object\n",
    "        h2_tag = soup.find('h2')\n",
    "\n",
    "        # Check if h2_tag is found before trying to find span_tag\n",
    "        if h2_tag:\n",
    "            span_tag = h1_tag.find('span', itemprop='author')\n",
    "\n",
    "            # Check if span_tag is found before accessing text\n",
    "            if span_tag:\n",
    "                return span_tag.text.strip()\n",
    "            else:\n",
    "                raise Exception(\"Unable to find span_tag with itemprop='author'\")\n",
    "        else:\n",
    "            raise Exception(\"Unable to find h2_tag\")\n",
    "\n",
    "    except Exception as e:\n",
    "        messagebox.showerror('错误', '找不到对应的小说作者。')\n",
    "        return\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
